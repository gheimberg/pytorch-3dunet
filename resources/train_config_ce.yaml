# use a fixed random seed to guarantee that when you run the code twice you will get the same outcome
manual_seed: 0
# model configuration
model:
  # model class
  name: UNet3D
  # number of input channels to the model
  in_channels: 1
  # number of output channels
  out_channels: 2
  # determines the order of operators in a single layer (crg - Conv3d+ReLU+GroupNorm)
  layer_order: crg
  # feature maps scale factor
  f_maps: 32
  # number of groups in the groupnorm
  num_groups: 8
  # apply element-wise nn.Sigmoid after the final 1x1 convolution, otherwise apply nn.Softmax
  final_sigmoid: false
# trainer configuration
trainer:
  # path to the checkpoint directory
  checkpoint_dir: 3dunet
  # path to latest checkpoint; if provided the training will be resumed from that checkpoint
  resume: null
  #resume: '3dunet/last_checkpoint.pytorch'
  # how many iterations between validations
  validate_after_iters: 20
  # how many iterations between tensorboard logging
  log_after_iters: 20
  # max number of epochs
  epochs: 2000
  # max number of iterations
  iters: 100000
  # model with higher eval score is considered better
  eval_score_higher_is_better: True
# optimizer configuration
optimizer:
  # initial learning rate
  learning_rate: 0.0002
  # weight decay
  weight_decay: 0.0001
# loss function configuration
loss:
  # loss function to be used during training
  name: WeightedCrossEntropyLoss
  # A manual rescaling weight given to each class.
  #loss_weight: null
  loss_weight: [.05,.95]
  # a target value that is ignored and does not contribute to the input gradient
  ignore_index: null
# evaluation metric configuration
eval_metric:
  name: MeanIoU
  # a target label that is ignored during metric evaluation
  ignore_index: null
lr_scheduler:
  name: MultiStepLR
  milestones: [10, 30, 60]
  gamma: 0.2
# data loaders configuration
loaders:
  # train patch size given to the network (adapt to fit in your GPU mem, generally the bigger patch the better)
  #train_patch: [32, 64, 64]
  train_patch: [128, 128, 128]  
  # train stride between patches
  #train_stride: [8, 16, 16]
  train_stride: [64, 64, 64]  
  # validation patch (can be bigger than train patch since there is no backprop)
  #val_patch: [32, 64, 64]
  val_patch: [128, 128, 128]  
  # validation stride (validation patches doesn't need to overlap)
  #val_stride: [32, 64, 64]
  val_stride: [128, 128, 128]  
  # path to the raw data within the H5
  raw_internal_path: raw
  # path to the the label data withtin the H5
  label_internal_path: label
  # paths to the training datasets
  train_path:
    #- '../suki_fractals/test.h5' 
    - '../h5_fractals/0.h5'
    - '../h5_fractals/1.h5'
    - '../h5_fractals/2.h5'    
    - '../h5_fractals/3.h5'    
    - '../h5_fractals/4.h5'
    - '../h5_fractals/5.h5'
    - '../h5_fractals/6.h5'    
    - '../h5_fractals/7.h5' 
    - '../h5_fractals/8.h5'
    - '../h5_fractals/9.h5'
    - '../h5_fractals/10.h5'
    - '../h5_fractals/11.h5'
    - '../h5_fractals/22.h5'
    - '../h5_fractals/23.h5'
    - '../h5_fractals/24.h5'    
    - '../h5_fractals/25.h5'    
    - '../h5_fractals/26.h5'
    - '../h5_fractals/27.h5'
    - '../h5_fractals/28.h5'    
    - '../h5_fractals/29.h5' 
    - '../h5_fractals/30.h5'
    - '../h5_fractals/31.h5'
    - '../h5_fractals/32.h5'
    - '../h5_fractals/33.h5'    
    - '../h5_fractals/34.h5'    
    - '../h5_fractals/35.h5'    
    - '../h5_fractals/36.h5'
    - '../h5_fractals/37.h5'
    - '../h5_fractals/38.h5'    
    - '../h5_fractals/39.h5' 
    - '../h5_fractals/40.h5'  
    - '../h5_fractals/41.h5'
    - '../h5_fractals/42.h5'
    - '../h5_fractals/43.h5'    
    - '../h5_fractals/44.h5'    
    - '../h5_fractals/45.h5'
    - '../h5_fractals/46.h5'
    - '../h5_fractals/47.h5'
    - '../h5_fractals/48.h5'    
    - '../h5_fractals/49.h5'  
    - '../h5_fractals/50.h5'  
    - '../h5_fractals/51.h5'
    - '../h5_fractals/52.h5'
    - '../h5_fractals/53.h5'    
    - '../h5_fractals/54.h5'    
    - '../h5_fractals/55.h5'
    - '../h5_fractals/56.h5'
    - '../h5_fractals/57.h5'
    - '../h5_fractals/58.h5'    
    - '../h5_fractals/59.h5'   
    - '../h5_fractals/60.h5'  
    - '../h5_fractals/61.h5'
    - '../h5_fractals/62.h5'
    - '../h5_fractals/63.h5'    
    - '../h5_fractals/64.h5'    
    - '../h5_fractals/65.h5'
    - '../h5_fractals/66.h5'
    - '../h5_fractals/67.h5'
    - '../h5_fractals/68.h5'    
    - '../h5_fractals/69.h5'      
    #- 'resources/random_label3D.h5'
    #- '../segment_trachea_test/0.h5'


  # paths to the validation datasets
  val_path:
    #- '../suki_fractals/test.h5' 
    #- 'resources/random_label3D.h5'
    #- '../h5_fractals/4.h5'
    #- '../segment_trachea_test/22.h5'
    #- '../segment_trachea_test/23.h5'
    #- '../segment_trachea_test/23.h5'
    - '../h5_fractals/70.h5'
    - '../h5_fractals/71.h5'
    - '../h5_fractals/72.h5'    
    - '../h5_fractals/73.h5' 
    - '../h5_fractals/74.h5'    
    - '../h5_fractals/75.h5'     

 
  # how many subprocesses to use for data loading
  num_workers: 12
  # data transformations/augmentations
  transformer:
    train:
      raw:
        - name: Normalize
        #- name: RandomFlip
        #- name: RandomRotate90
        #- name: RandomRotate
          # rotate only in ZY only
          #axes: [[2, 1]]
          #angle_spectrum: 15
          #mode: reflect
        #- name: ElasticDeformation
          #spline_order: 3
        #- name: RandomContrast
        - name: ToTensor
          expand_dims: true
      label:
        #- name: RandomFlip
        #- name: RandomRotate90
        #- name: RandomRotate
          # rotate only in ZY only
          #axes: [[2, 1]]
          #angle_spectrum: 15
          #mode: reflect
        #- name: ElasticDeformation
          #spline_order: 0
        - name: ToTensor
          expand_dims: false
          dtype: 'long'
    test:
      raw:
        - name: Normalize
        - name: ToTensor
          expand_dims: true
      label:
        - name: ToTensor
          expand_dims: false
          dtype: 'long'
