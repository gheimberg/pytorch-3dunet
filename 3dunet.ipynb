{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from datasets.hdf5 import get_train_loaders\n",
    "from unet3d.config import load_config\n",
    "from unet3d.losses import get_loss_criterion\n",
    "from unet3d.metrics import get_evaluation_metric\n",
    "from unet3d.model import get_model\n",
    "from unet3d.trainer import UNet3DTrainer\n",
    "from unet3d.utils import get_logger\n",
    "from unet3d.utils import get_number_of_learnable_parameters\n",
    "\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-10 15:19:51,585 [MainThread] INFO UNet3DTrainer - {'manual_seed': 0, 'model': {'name': 'UNet3D', 'in_channels': 1, 'out_channels': 2, 'layer_order': 'crg', 'f_maps': 32, 'num_groups': 8, 'final_sigmoid': False}, 'trainer': {'checkpoint_dir': '3dunet', 'resume': None, 'validate_after_iters': 20, 'log_after_iters': 20, 'epochs': 50, 'iters': 100000, 'eval_score_higher_is_better': True}, 'optimizer': {'learning_rate': 0.0002, 'weight_decay': 0.0001}, 'loss': {'name': 'CrossEntropyLoss', 'loss_weight': None, 'ignore_index': None}, 'eval_metric': {'name': 'MeanIoU', 'ignore_index': None}, 'lr_scheduler': {'name': 'MultiStepLR', 'milestones': [10, 30, 60], 'gamma': 0.2}, 'loaders': {'train_patch': [32, 64, 64], 'train_stride': [8, 16, 16], 'val_patch': [32, 64, 64], 'val_stride': [32, 64, 64], 'raw_internal_path': 'raw', 'label_internal_path': 'label', 'train_path': ['resources/random_label3D.h5'], 'val_path': ['resources/random_label3D.h5'], 'num_workers': 8, 'transformer': {'train': {'raw': [{'name': 'Normalize'}, {'name': 'RandomFlip'}, {'name': 'RandomRotate90'}, {'name': 'RandomRotate', 'axes': [[2, 1]], 'angle_spectrum': 15, 'mode': 'reflect'}, {'name': 'ElasticDeformation', 'spline_order': 3}, {'name': 'RandomContrast'}, {'name': 'ToTensor', 'expand_dims': True}], 'label': [{'name': 'RandomFlip'}, {'name': 'RandomRotate90'}, {'name': 'RandomRotate', 'axes': [[2, 1]], 'angle_spectrum': 15, 'mode': 'reflect'}, {'name': 'ElasticDeformation', 'spline_order': 0}, {'name': 'ToTensor', 'expand_dims': False, 'dtype': 'long'}]}, 'test': {'raw': [{'name': 'Normalize'}, {'name': 'ToTensor', 'expand_dims': True}], 'label': [{'name': 'ToTensor', 'expand_dims': False, 'dtype': 'long'}]}}}, 'device': device(type='cpu')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/lung/lib/python3.7/site-packages/ipykernel_launcher.py:13: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "# set config and device parameters\n",
    "\n",
    "logger = get_logger('UNet3DTrainer')\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEFAULT_DEVICE = 'cuda:0'\n",
    "else:\n",
    "     DEFAULT_DEVICE = 'cpu'\n",
    "\n",
    "# config file\n",
    "config_file = 'resources/train_config_ce.yaml'\n",
    "config = yaml.load(open(config_file, 'r'))\n",
    "# Get a device to train on\n",
    "device = config.get('device', DEFAULT_DEVICE)\n",
    "config['device'] = torch.device(device)\n",
    "\n",
    "logger.info(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_trainer(config, model, optimizer, lr_scheduler, loss_criterion, eval_criterion, loaders, logger):\n",
    "    assert 'trainer' in config, 'Could not find trainer configuration'\n",
    "    trainer_config = config['trainer']\n",
    "\n",
    "    resume = trainer_config.get('resume', None)\n",
    "    pre_trained = trainer_config.get('pre_trained', None)\n",
    "\n",
    "    if resume is not None:\n",
    "        # continue training from a given checkpoint\n",
    "        return UNet3DTrainer.from_checkpoint(resume, model,\n",
    "                                             optimizer, lr_scheduler, loss_criterion,\n",
    "                                             eval_criterion, loaders,\n",
    "                                             logger=logger)\n",
    "    elif pre_trained is not None:\n",
    "        # fine-tune a given pre-trained model\n",
    "        return UNet3DTrainer.from_pretrained(pre_trained, model, optimizer, lr_scheduler, loss_criterion,\n",
    "                                             eval_criterion, device=config['device'], loaders=loaders,\n",
    "                                             max_num_epochs=trainer_config['epochs'],\n",
    "                                             max_num_iterations=trainer_config['iters'],\n",
    "                                             validate_after_iters=trainer_config['validate_after_iters'],\n",
    "                                             log_after_iters=trainer_config['log_after_iters'],\n",
    "                                             eval_score_higher_is_better=trainer_config['eval_score_higher_is_better'],\n",
    "                                             logger=logger)\n",
    "    else:\n",
    "        # start training from scratch\n",
    "        return UNet3DTrainer(model, optimizer, lr_scheduler, loss_criterion, eval_criterion,\n",
    "                             config['device'], loaders, trainer_config['checkpoint_dir'],\n",
    "                             max_num_epochs=trainer_config['epochs'],\n",
    "                             max_num_iterations=trainer_config['iters'],\n",
    "                             validate_after_iters=trainer_config['validate_after_iters'],\n",
    "                             log_after_iters=trainer_config['log_after_iters'],\n",
    "                             eval_score_higher_is_better=trainer_config['eval_score_higher_is_better'],\n",
    "                             logger=logger)\n",
    "\n",
    "\n",
    "def _create_optimizer(config, model):\n",
    "    assert 'optimizer' in config, 'Cannot find optimizer configuration'\n",
    "    optimizer_config = config['optimizer']\n",
    "    learning_rate = optimizer_config['learning_rate']\n",
    "    weight_decay = optimizer_config['weight_decay']\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def _create_lr_scheduler(config, optimizer):\n",
    "    lr_config = config.get('lr_scheduler', None)\n",
    "    if lr_config is None:\n",
    "        # use ReduceLROnPlateau as a default scheduler\n",
    "        return ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=20, verbose=True)\n",
    "    else:\n",
    "        class_name = lr_config.pop('name')\n",
    "        m = importlib.import_module('torch.optim.lr_scheduler')\n",
    "        clazz = getattr(m, class_name)\n",
    "        # add optimizer to the config\n",
    "        lr_config['optimizer'] = optimizer\n",
    "        return clazz(**lr_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-10 15:19:53,382 [MainThread] INFO UNet3DTrainer - Seed the RNG for all devices with 0\n",
      "2019-07-10 15:19:53,439 [MainThread] INFO UNet3DTrainer - Sending the model to 'cpu'\n",
      "2019-07-10 15:19:53,442 [MainThread] INFO UNet3DTrainer - Number of learnable params 4080914\n",
      "2019-07-10 15:19:53,444 [MainThread] INFO HDF5Dataset - Creating training and validation set loaders...\n",
      "2019-07-10 15:19:53,444 [MainThread] INFO HDF5Dataset - Slice builder class: SliceBuilder\n",
      "2019-07-10 15:19:53,445 [MainThread] INFO HDF5Dataset - Loading training set from: resources/random_label3D.h5...\n",
      "2019-07-10 15:19:53,608 [MainThread] INFO HDF5Dataset - Loading validation set from: resources/random_label3D.h5...\n",
      "2019-07-10 15:19:53,758 [MainThread] INFO HDF5Dataset - Number of workers for train/val datasets: 8\n",
      "2019-07-10 15:19:53,759 [MainThread] INFO UNet3DTrainer - UNet3D(\n",
      "  (encoders): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (basic_module): DoubleConv(\n",
      "        (SingleConv1): SingleConv(\n",
      "          (conv): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace)\n",
      "          (groupnorm): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (SingleConv2): SingleConv(\n",
      "          (conv): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace)\n",
      "          (groupnorm): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (pooling): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      (basic_module): DoubleConv(\n",
      "        (SingleConv1): SingleConv(\n",
      "          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace)\n",
      "          (groupnorm): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (SingleConv2): SingleConv(\n",
      "          (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace)\n",
      "          (groupnorm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Encoder(\n",
      "      (pooling): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      (basic_module): DoubleConv(\n",
      "        (SingleConv1): SingleConv(\n",
      "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace)\n",
      "          (groupnorm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (SingleConv2): SingleConv(\n",
      "          (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace)\n",
      "          (groupnorm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Encoder(\n",
      "      (pooling): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      (basic_module): DoubleConv(\n",
      "        (SingleConv1): SingleConv(\n",
      "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace)\n",
      "          (groupnorm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (SingleConv2): SingleConv(\n",
      "          (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace)\n",
      "          (groupnorm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoders): ModuleList(\n",
      "    (0): Decoder(\n",
      "      (basic_module): DoubleConv(\n",
      "        (SingleConv1): SingleConv(\n",
      "          (conv): Conv3d(384, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace)\n",
      "          (groupnorm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (SingleConv2): SingleConv(\n",
      "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace)\n",
      "          (groupnorm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Decoder(\n",
      "      (basic_module): DoubleConv(\n",
      "        (SingleConv1): SingleConv(\n",
      "          (conv): Conv3d(192, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace)\n",
      "          (groupnorm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (SingleConv2): SingleConv(\n",
      "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace)\n",
      "          (groupnorm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Decoder(\n",
      "      (basic_module): DoubleConv(\n",
      "        (SingleConv1): SingleConv(\n",
      "          (conv): Conv3d(96, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace)\n",
      "          (groupnorm): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (SingleConv2): SingleConv(\n",
      "          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace)\n",
      "          (groupnorm): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_conv): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "  (final_activation): Softmax()\n",
      ")\n",
      "2019-07-10 15:19:53,760 [MainThread] INFO UNet3DTrainer - eval_score_higher_is_better: True\n",
      "2019-07-10 15:19:54,298 [MainThread] INFO UNet3DTrainer - Training iteration 1. Batch 0. Epoch [0/49]\n",
      "2019-07-10 15:20:05,294 [MainThread] INFO UNet3DTrainer - Training iteration 2. Batch 1. Epoch [0/49]\n",
      "2019-07-10 15:20:12,521 [MainThread] INFO UNet3DTrainer - Training iteration 3. Batch 2. Epoch [0/49]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda3/envs/lung/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and log experiment configuration\n",
    "manual_seed = config.get('manual_seed', None)\n",
    "if manual_seed is not None:\n",
    "    logger.info(f'Seed the RNG for all devices with {manual_seed}')\n",
    "    torch.manual_seed(manual_seed)\n",
    "    # see https://pytorch.org/docs/stable/notes/randomness.html\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Create the model\n",
    "model = get_model(config)\n",
    "\n",
    "# put the model on GPUs (if available)\n",
    "#if torch.cuda.is_available():\n",
    "logger.info(f\"Sending the model to '{config['device']}'\")\n",
    "model = model.to(config['device'])\n",
    "                \n",
    "# Log the number of learnable parameters\n",
    "logger.info(f'Number of learnable params {get_number_of_learnable_parameters(model)}')\n",
    "\n",
    "# Create loss criterion\n",
    "loss_criterion = get_loss_criterion(config)\n",
    "# Create evaluation metric\n",
    "eval_criterion = get_evaluation_metric(config)\n",
    "\n",
    "# Create data loaders\n",
    "loaders = get_train_loaders(config)\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = _create_optimizer(config, model)\n",
    "\n",
    "# Create learning rate adjustment strategy\n",
    "lr_scheduler = _create_lr_scheduler(config, optimizer)\n",
    "\n",
    "# Create model trainer\n",
    "trainer = _create_trainer(config, model=model, optimizer=optimizer, lr_scheduler=lr_scheduler,\n",
    "                          loss_criterion=loss_criterion, eval_criterion=eval_criterion, loaders=loaders,\n",
    "                          logger=logger)\n",
    "# Start training\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lung",
   "language": "python",
   "name": "lung"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
